import { FileWithPreview, AnalysisResult } from "../types";

// API Configuration
// In production on Vercel, use relative URLs (same domain)
// In development, use localhost or the env variable
const API_BASE_URL = import.meta.env.VITE_API_BASE_URL || 
  (import.meta.env.PROD ? '' : 'http://localhost:4000');

// Internal state for chat session management
let analysisContext: AnalysisResult | null = null;
let chatHistory: Array<{ role: 'user' | 'model'; text: string }> = [];

// Vercel Pro plan has higher limits but we still batch for reliability
// Average document image is ~300-500KB in base64
const MAX_BATCH_SIZE = 5; // Smaller batches = more reliable
const MAX_PAYLOAD_SIZE = 3 * 1024 * 1024; // 3MB conservative limit
const MAX_SINGLE_FILE_SIZE = 2.5 * 1024 * 1024; // 2.5MB per file max

/**
 * Analyze a single batch of documents
 */
const analyzeBatch = async (documents: Array<{ name: string; mimeType: string; data: string }>): Promise<AnalysisResult> => {
  // Add metadata about document count for detection
  const requestBody = {
    documents,
    metadata: {
      totalDocuments: documents.length,
      documentNames: documents.map(d => d.name),
    }
  };

  const response = await fetch(`${API_BASE_URL}/api/analyze`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(requestBody),
  });

  if (!response.ok) {
    if (response.status === 413) {
      throw new Error("File too large for analysis. Please upload a smaller PDF (under 5MB) or take photos of individual pages and upload as images.");
    }
    if (response.status === 504 || response.status === 502) {
      throw new Error("Analysis timed out. Please try with fewer pages or wait a moment and try again.");
    }
    const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
    throw new Error(errorData.error || `Server error: ${response.status}`);
  }

  return await response.json() as AnalysisResult;
};

/**
 * Merge multiple analysis results into one comprehensive result
 * CRITICAL: Check if results indicate different deeds were analyzed
 */
const mergeAnalysisResults = (results: AnalysisResult[]): AnalysisResult => {
  if (results.length === 0) {
    throw new Error("No results to merge");
  }

  if (results.length === 1) {
    return results[0];
  }

  // Aggregate all results FIRST (before using them)
  const allGoodPoints = new Set<string>();
  const allBadPoints = new Set<string>();
  const allCriticalIssues = new Set<string>();
  const allMissingInfo = new Set<string>();
  const allNextSteps = new Set<string>();
  const allTimelineEvents: Array<{ date: string; event: string }> = [];

  // CRITICAL CHECK: Detect if different batches analyzed different deeds
  // Compare deed numbers, dates, and locations from summaries
  const deedIdentifiers = results.map(r => ({
    deedNo: r.summary?.deedNo || '',
    date: r.summary?.date || '',
    mouza: r.summary?.mouza || '',
  }));

  const uniqueDeeds = new Set(
    deedIdentifiers
      .filter(d => d.deedNo || d.date || d.mouza) // Only count if has identifier
      .map(d => `${d.deedNo}|${d.date}|${d.mouza}`)
  );

  const hasMultipleDeeds = uniqueDeeds.size > 1;

  // CRITICAL: If multiple different deeds detected, add warning and increase risk
  if (hasMultipleDeeds) {
    console.warn('âš ï¸ CRITICAL: Multiple different deeds detected in analysis!');
    allCriticalIssues.add('âš ï¸ à¦¸à¦¤à¦°à§à¦•à¦¤à¦¾: à¦à¦•à¦¾à¦§à¦¿à¦• à¦­à¦¿à¦¨à§à¦¨ à¦¦à¦²à¦¿à¦² à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦†à¦ªà¦²à§‹à¦¡ à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡à¥¤ à¦à¦Ÿà¦¿ à¦…à¦¤à§à¦¯à¦¨à§à¦¤ à¦à§à¦à¦•à¦¿à¦ªà§‚à¦°à§à¦£à¥¤ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¦à¦²à¦¿à¦² à¦†à¦²à¦¾à¦¦à¦¾à¦­à¦¾à¦¬à§‡ à¦¬à¦¿à¦¶à§à¦²à§‡à¦·à¦£ à¦•à¦°à¦¾ à¦‰à¦šà¦¿à¦¤à¥¤ à¦­à¦¿à¦¨à§à¦¨ à¦¦à¦²à¦¿à¦² à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¬à¦¿à¦¶à§à¦²à§‡à¦·à¦£ à¦•à¦°à¦²à§‡ à¦­à§à¦² à¦«à¦²à¦¾à¦«à¦² à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼ à¦¯à¦¾ à¦†à¦‡à¦¨à¦¿ à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦¸à§ƒà¦·à§à¦Ÿà¦¿ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤');
  }

  let totalRiskScore = 0;
  let maxRiskLevel: AnalysisResult['riskLevel'] = 'Safe';
  const riskLevelOrder = { 'Safe': 0, 'Low Risk': 1, 'Medium Risk': 2, 'High Risk': 3, 'Critical': 4 };

  results.forEach(result => {
    result.goodPoints.forEach(p => allGoodPoints.add(p));
    result.badPoints.forEach(p => allBadPoints.add(p));
    result.criticalIssues.forEach(p => allCriticalIssues.add(p));
    result.missingInfo.forEach(p => allMissingInfo.add(p));
    result.nextSteps.forEach(p => allNextSteps.add(p));
    allTimelineEvents.push(...result.chainOfTitleTimeline);
    totalRiskScore += result.riskScore;
    
    if (riskLevelOrder[result.riskLevel] > riskLevelOrder[maxRiskLevel]) {
      maxRiskLevel = result.riskLevel;
    }
  });

  // Use the first result as base, but aggregate all data
  const baseResult = results[0];
  let avgRiskScore = Math.round(totalRiskScore / results.length);
  
  // CRITICAL: If multiple deeds detected, significantly increase risk score
  if (hasMultipleDeeds) {
    avgRiskScore = Math.min(100, avgRiskScore + 35); // Add 35 points for mixed deeds
    maxRiskLevel = 'Critical'; // Force to Critical level
    console.warn('âš ï¸ Risk score increased due to multiple deeds:', avgRiskScore);
  }

  return {
    ...baseResult,
    riskScore: avgRiskScore,
    riskLevel: maxRiskLevel,
    documentType: results.length > 1 ? `${results.length} Documents` : baseResult.documentType,
    goodPoints: Array.from(allGoodPoints),
    badPoints: Array.from(allBadPoints),
    criticalIssues: Array.from(allCriticalIssues),
    missingInfo: Array.from(allMissingInfo),
    nextSteps: Array.from(allNextSteps),
    chainOfTitleTimeline: allTimelineEvents,
    chainOfTitleAnalysis: results.map(r => r.chainOfTitleAnalysis).join('\n\n'),
  };
};

/**
 * Analyze property documents with batch processing for large file sets
 */
export const analyzeDocuments = async (
  files: FileWithPreview[],
  onProgress?: (current: number, total: number, currentBatch: number, totalBatches: number) => void
): Promise<AnalysisResult> => {
  try {
    // Convert FileWithPreview[] to backend DocumentInput format
    const allDocuments = files
      .filter(file => file.base64Data)
      .map(file => {
        // Strip the data URL prefix if present to get raw base64
        const cleanBase64 = file.base64Data!.includes(',') 
          ? file.base64Data!.split(',')[1] 
          : file.base64Data!;
        
        return {
          name: file.file.name,
          mimeType: file.mimeType,
          data: cleanBase64
        };
      });

    if (allDocuments.length === 0) {
      throw new Error("No valid documents provided");
    }

    // Check for oversized files and warn user
    const oversizedFiles = allDocuments.filter(doc => doc.data.length > MAX_SINGLE_FILE_SIZE);
    if (oversizedFiles.length > 0) {
      console.warn(`âš ï¸ ${oversizedFiles.length} files exceed size limit:`, oversizedFiles.map(f => f.name));
      // For PDFs, we still try to process them but one at a time
    }

    // Smart batching: group files by size to maximize throughput while staying under limit
    const batches: Array<{ name: string; mimeType: string; data: string }[]> = [];
    let currentBatch: Array<{ name: string; mimeType: string; data: string }> = [];
    let currentBatchSize = 0;

    for (const doc of allDocuments) {
      const docSize = doc.data.length;
      
      // If single file is too large, put it in its own batch
      if (docSize > MAX_SINGLE_FILE_SIZE) {
        // Push current batch first if not empty
        if (currentBatch.length > 0) {
          batches.push(currentBatch);
          currentBatch = [];
          currentBatchSize = 0;
        }
        // Put oversized file in its own batch (we'll try anyway)
        batches.push([doc]);
        console.log(`ðŸ“¦ Large file "${doc.name}" (${(docSize / 1024 / 1024).toFixed(1)}MB) in separate batch`);
        continue;
      }
      
      // If adding this doc would exceed limit OR batch is full, start new batch
      if (currentBatchSize + docSize > MAX_PAYLOAD_SIZE || currentBatch.length >= MAX_BATCH_SIZE) {
        if (currentBatch.length > 0) {
          batches.push(currentBatch);
        }
        currentBatch = [doc];
        currentBatchSize = docSize;
      } else {
        currentBatch.push(doc);
        currentBatchSize += docSize;
      }
    }
    
    // Don't forget the last batch
    if (currentBatch.length > 0) {
      batches.push(currentBatch);
    }
    
    console.log(`ðŸ“Š Created ${batches.length} batches from ${allDocuments.length} documents`);

    const results: AnalysisResult[] = [];
    const totalBatches = batches.length;
    let processedFiles = 0;

    // Process batches with controlled concurrency (2 at a time for speed)
    const CONCURRENT_BATCHES = 2;
    
    for (let i = 0; i < batches.length; i += CONCURRENT_BATCHES) {
      const batchGroup = batches.slice(i, i + CONCURRENT_BATCHES);
      
      // Report progress
      if (onProgress) {
        onProgress(processedFiles, allDocuments.length, i + 1, totalBatches);
      }

      // Process batch group in parallel
      const batchPromises = batchGroup.map(async (batch, idx) => {
        try {
          const result = await analyzeBatch(batch);
          return { success: true, result, batchIndex: i + idx };
        } catch (error: any) {
          console.error(`Batch ${i + idx + 1} failed:`, error);
          return { success: false, error, batchIndex: i + idx };
        }
      });

      const batchResults = await Promise.all(batchPromises);
      
      for (const br of batchResults) {
        if (br.success && br.result) {
          results.push(br.result);
        }
        processedFiles += batches[br.batchIndex]?.length || 0;
      }

      // Small delay between batch groups to avoid rate limiting
      if (i + CONCURRENT_BATCHES < batches.length) {
        await new Promise(resolve => setTimeout(resolve, 300));
      }
    }

    if (results.length === 0) {
      throw new Error("All batches failed to process");
    }

    // Merge all batch results into one comprehensive result
    return mergeAnalysisResults(results);

  } catch (error: any) {
    console.error("Analysis error:", error);
    
    // Provide user-friendly error messages
    if (error.message) {
      throw error;
    }
    
    if (error instanceof TypeError && error.message.includes('fetch')) {
      throw new Error("Unable to connect to the server. Please check your connection and ensure the backend is running.");
    }
    
    throw new Error("Failed to analyze documents. Please try again.");
  }
};

/**
 * Initialize chat session with analysis context
 * This stores the context for subsequent chat messages
 * The backend will initialize the chat when the first message is sent
 */
export const startChatSession = async (context: AnalysisResult): Promise<void> => {
  analysisContext = context;
  chatHistory = [];
  // No API call needed - context will be sent with first message
};

/**
 * Send a message to the AI chat assistant
 * Maintains backward compatibility with existing component usage
 */
export const sendMessageToAI = async (message: string): Promise<string> => {
  try {
    if (!analysisContext) {
      throw new Error("Chat session not initialized. Please call startChatSession first.");
    }

    // Only send analysisContext on first message (when history is empty)
    const requestBody: {
      history: Array<{ role: 'user' | 'model'; text: string }>;
      input: string;
      analysisContext?: AnalysisResult;
    } = {
      history: chatHistory,
      input: message,
    };

    if (chatHistory.length === 0) {
      requestBody.analysisContext = analysisContext;
    }

    const response = await fetch(`${API_BASE_URL}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
      throw new Error(errorData.error || `Server error: ${response.status}`);
    }

    const result = await response.json();
    
    // Update internal history state
    chatHistory = result.updatedHistory;
    
    return result.reply;

  } catch (error: any) {
    console.error("Chat message error:", error);
    
    if (error.message) {
      throw error;
    }
    
    if (error instanceof TypeError && error.message.includes('fetch')) {
      throw new Error("Unable to connect to the server. Please check your connection and ensure the backend is running.");
    }
    
    throw new Error("Failed to send message. Please try again.");
  }
};
